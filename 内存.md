# 内存寻址
## 三种地址

## 分段与分页

## 页表初始化：内核页表，进程页表

硬件高速缓存

TLB

# 内存管理

RAM的某些部分永久分配给内核，并用来存放内核代码以及静态内核数据结构。
RAM的其余部分称为动态内存，需要时分配，不需要时释放。

## 页框管理
Linux采用4KB页框大小作为标准的内存分配单元。

### 页描述符
为了根据需要分配动态内存，内核必须能区分哪些页框包含的是属于**进程**的页，哪些包含的是**内核代码或内核数据**；内核还必须能确定动态内存中的页框**是否空闲**。

因此，内核必须记录每个页框当前的状态——使用**页描述符**。

页描述符的类型为struct page,所有的页描述符存放在mem_map数组中。

virt_to_page(addr)得到线性地址addr对应的页描述符地址。

`
#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
`
__pa(kaddr)宏只能将固定映射到内核空间的线性地址转换成物理地址。

pfn_to_page(pfn)产生页框号pfn对应的页描述符地址。

#define page_to_pfn __page_to_pfn

__page_to_pfn这个宏的实现与内存管理模型有关。

比如flat memory model:访问物理内存的时候，物理地址空间是一个连续的，没有空洞的地址空间;
物理地址跟线性地址空间就有简单的线性映射关系，struct page数组mem_map中的每一项都可以对应某个具体的物理页面；

在这种模型下，

`
#define __pfn_to_page(pfn)	(mem_map + ((pfn) - ARCH_PFN_OFFSET))
`
ARCH_PFN_OFFSET 跟架构相关的物理起始地址的PFN，也就是physical_start_address>>PAGE_SHIFT。

## 内存管理模型
linux内存模型有三种：flat memory model，Discontiguous Memory Model，Sparse Memory Model。

分别由参数CONFIG_FLATMEM，CONFIG_DISCONTIGMEM，CONFIG_SPARSEMEM确定。

具体定义在include/asm-generic/memory-model.h中；

memory model针对的是物理内存的分布，主要涉及PFN跟page结构的转换。

### flat memory model

之前说过，flat memory model的物理地址空间是连续的。在这种模型中，页描述符和页框号的相互转换如下：

`
#define __pfn_to_page(pfn)	(mem_map + ((pfn) - ARCH_PFN_OFFSET))
#define __page_to_pfn(page)	((unsigned long)((page) - mem_map) + \
				 ARCH_PFN_OFFSET)
`

### Discontiguous Memory Model
而Discontiguous Memory Model不同，它的地址空间有一些空洞，是**不连续**的;Discontiguous Memory Model可以看做是flat memory model的扩展，单个连续的物理地址空间作为一个**node**，按flat memory model管理。

这是为了支持**非一致内存访问(Non-Uniform Memory Access , NUMA)模型**：在这种模型中，给定CPU对不同内存单元的访问时间可能不一样。

比如某些多处理器Alpha或MIPS计算机，就采用了非一致内存访问模型。（注意：NUMA模型是针对某一种体系结构的计算机而言的，而内存模型是针对内核。）

而在Discontiguous Memory Model中，系统的物理内存被划分为几个节点（node），划分的依据就是cpu访问这片内存的时间。

所以，在NUMA模型中，如果使用Discontiguous Memory Model，就可以将某个CPU最常引用的内核数据结构放到一个对它而言访问时间较少的节点中，从而达到减少访问内存所需时间的目的。

80x86体系结构使用的是一致访问内存(UMA)模型，所以它并不真正需要NUMA的支持。内核可以直接采用flat memory model。

但事实上，即便是flat memory model，Linux也使用了节点：一个包含系统中所有物理内存的节点。

这看起来是多此一举，其实不然：内核假定在所有的体系结构中物理内存都被划分为一个或多个节点，从而让内存代码的处理有了更好的可移植性。

(Linux对页表的处理也采用了类似的方法，即便硬件体系结构仅仅定义了两级页表，Linux也使用四级。）

### Sparse Memory Model

为了支持内存hotplug，引入了sparse memory model，热插拔导致了一个node上的内存可能都变得更加"稀疏";

更多关于内存模型的具体信息[这篇博客](https://blog.csdn.net/u014089131/article/details/53095985)

## 内存管理区
80x86 UMA体系结构中的内存分为3个管理区。

ZONE_DMA,包含低于16MB的内存页框

ZONE_NORMAL,包含高于16MB，低于896MB的内存页框

ZONE_HIHMEM，包含高于896MB的内存页框

管理区声明在include/linux/mmzone.h中。
为什么要分成这几个管理区?

因为计算机体系结构有硬件的制约，限制了页框可以使用的方式。

这三个管理区就是为了解决80x86的两种硬件制约。
1. ISA总线的直接内存存取（DMA）处理器只能对RAM的前16MB寻址。
2. 在具有大容量RAM的现代32位计算机中，CPU不能直接访问所有的物理内存。

ZONE_DMA包含的页框可以由老式基于ISA的设备通过DMA使用。

ZONE_DMA和ZONE_NORMAL包含内存的“常规”页框，内核在初始化时，会把它们线性得映射到线性地址空间的第4个GB，所以内核可以直接访问。

而ZONE_HIGHMEM包含的内存页不能由内核直接访问。

而对于64位体系结构，ZONE_HIGHMEM区总为空。（涉及高端内存的映射，后面再来解释）

而且x86_64还多了一个ZONE_DMA32,因为它支持32位地址总线的DMA内存空间。它的范围是低于4GB的内存空间。

每个**页描述符**都有到内存节点和到内存管理区的链接。为了节省空间，没有采用指针，而是被编码成索引存放在flags字段的高位。

获得内存节点号与内存管理区号的函数如下：

`
static inline int page_to_nid(const struct page *page)
{
	return (page->flags >> NODES_PGSHIFT) & NODES_MASK;
}

static inline enum zone_type page_zonenum(const struct page *page)
{
#ifdef CONFIG_ZONE_DEVICE
	if (page->flags & ZONE_DEVICE_FLAG)
		return ZONE_DEVICE;
#endif
	return (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;
}
`

## 保留的页框池

分配内存有两种不同的方法。

首先，这两种方法中相同的部分是，如果有足够的空闲内存可用，请求就会被立刻满足。

它们的不同之处在于，如果没有足够的空闲内存怎么处理。

1. 普通的内存分配请求：回收一些内存，并且将发出请求的内核控制路径阻塞，直到内存被释放。
2. 原子内存分配请求：原子请求从不被阻塞，如果没有足够的空闲页，仅仅是分配失败而已。（处理中断或执行临界区内的代码时，就会产生原子内存分配请求）

为了减少原子内存分配失败的可能性，内核为原子内存分配请求保留了一个页框池，只有在内存不足时才使用。

保留内存只来自于ZONE_DMA和ZONE_NORMAL，并且获得与各管理区的页框数目与两个管理区的相对大小成比例。

保留内存的数量（以KB为单位）存放在min_free_kbytes变量中。

`
int min_free_kbytes = 1024;
`

管理区描述符的page_min字段存储管理区内保留页框的数目。它和pages_low，pages_high字段一起还在页框回收算法中起作用。//3.10.0中好像没有这几个字段

## 分区页框分配器

内核中有一个被称为**分区页框分配器**的子系统，处理对连续页框组的内存分配请求。

它由三部分组成，管理区分配器，伙伴系统，每CPU页框高速缓存

管理区分配器：接受动态内存分配与释放的请求。在请求分配的情况下，该部分搜索一个能满足所请求的一组连续页框内存的管理区。

伙伴系统：在每个管理区内，**伙伴系统**处理连续页框。

每CPU高速缓存：为了提高系统性能，一小部分页框保留在高速缓存中，用于快速满足对单个页框的分配请求。（与伙伴系统同级）

### 请求和释放页框

6个函数和宏请求页框。


4个函数和宏释放页框。
### 高端内存页框的内核映射

内核无法直接访问没有被映射到内核线性空间的第4个GB的高端内存，在x86中，也就是896MB以上的页框，ZONE_HIGHMEM内存管理区内的页框。（在x86_64中，ZONE_HIGHMEM永远为空）

所以，在80X86体系结构中，不能使用返回所分配页框线性地址的页分配器函数来分配高端内存。

而是使用alloc_pages()与alloc_page()这两个返回第一个被分配页框的页描述符的线性地址的页分配函数。

分配之后还要可以通过线性地址对页进行访问才行。

内核线性地址空间的最后128MB中的一部分专门用于映射高端内存页框。这种映射是暂时的，即重复使用线性地址，使得整个高端内存能够在不同的时间被访问。

内核采用三种不同的机制将页框映射到高端内存，分别是永久内核映射，临时内核映射，非连续内存分配。
### 永久内核映射
永久内核映射允许内核建立高端页框到内核地址空间的长期映射。

它们使用主内核页表中一个专门的页表，其地址存放在pkmap_page_table变量中，有LAST_PKMAP个表项。（表项数取决于PAE是否被激活，如果被激活，就是1024项，否则是512项）

`
pte_t * pkmap_page_table;
`
该页表映射的线性地址从PKMAP_BASE开始。

pkmap_count数组包含LAST_PKMAP个计数器，pkmap_page_table页表中每一项都有一个。

`
static int pkmap_count[LAST_PKMAP];
`
1. 计数器为0表示对应的页表项没有映射任何高端内存页框，是可用的。
2. 计数器为1表示对应的页表项没有映射任何高端内存页框，但不可用，因为自从它最后一次使用以来，其相应的TLB表项还未被刷新。
3. 计数器为n（大于1）表示对应的页表项映射一个高端内存页框，并且正好有n-1个内核成分在使用这个页框。

为了记录高端内存页框与永久内核映射包含的线性地址之间的联系，内核使用了page_address_htbale哈希表。
该表包含一个page_address_map数据结构，用于为高端内存中的每一个页框进行当前映射。

page_address()函数返回页框对应的线性地址。如果页框在高端内存中并且没有映射，则返回NULL。

`
void *page_address(const struct page *page)
{
	unsigned long flags;
	void *ret;
	struct page_address_slot *pas;

	if (!PageHighMem(page))
		return lowmem_page_address(page);

	pas = page_slot(page);
	ret = NULL;
	spin_lock_irqsave(&pas->lock, flags);
	if (!list_empty(&pas->lh)) {
		struct page_address_map *pam;

		list_for_each_entry(pam, &pas->lh, list) {
			if (pam->page == page) {
				ret = pam->virtual;
				goto done;
			}
		}
	}
done:
	spin_unlock_irqrestore(&pas->lock, flags);
	return ret;
}
`
该函数接受一个页描述符指针作为参数。
1. 如果页框不在高端内存中，那么线性地址总是存在，可以通过lowmem_page_address()函数得到。

`
static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
}
`

2. 如果页框在高端内存中，就到page_address_htable哈希表中查找页框。如果找到，就返回它的线性地址，否则返回NULL。

kmap()函数建立永久映射

`
void *kmap(struct page *page)
{
	might_sleep();
	if (!PageHighMem(page))
		return page_address(page);
	return kmap_high(page);
}
`
如果页框属于高端内存，调用kmap_high()函数。
`
void *kmap_high(struct page *page)
{
	unsigned long vaddr;

	/*
	 * For highmem pages, we can't trust "virtual" until
	 * after we have the lock.
	 */
	lock_kmap();
	vaddr = (unsigned long)page_address(page);
	if (!vaddr)
		vaddr = map_new_virtual(page);
	pkmap_count[PKMAP_NR(vaddr)]++;
	BUG_ON(pkmap_count[PKMAP_NR(vaddr)] < 2);
	unlock_kmap();
	return (void*) vaddr;
}
`
该函数没有禁止中断，因为中断处理程序和可延迟函数不能调用kmap()。

如果页框之前没有被映射过，就调用map_new_virtual()函数将页框的物理地址插入到pkmap_page_table的一个项中并在page_address_htable哈希表中加入一个元素。

`

static inline unsigned long map_new_virtual(struct page *page)
{
	unsigned long vaddr;
	int count;

start:
	count = LAST_PKMAP;
	/* Find an empty entry */
	for (;;) {
		last_pkmap_nr = (last_pkmap_nr + 1) & LAST_PKMAP_MASK;
		if (!last_pkmap_nr) {
			flush_all_zero_pkmaps();
			count = LAST_PKMAP;
		}
		if (!pkmap_count[last_pkmap_nr])
			break;	/* Found a usable entry */
		if (--count)
			continue;

		/*
		 * Sleep for somebody else to unmap their entries
		 */
		{
			DECLARE_WAITQUEUE(wait, current);

			__set_current_state(TASK_UNINTERRUPTIBLE);
			add_wait_queue(&pkmap_map_wait, &wait);
			unlock_kmap();
			schedule();
			remove_wait_queue(&pkmap_map_wait, &wait);
			lock_kmap();

			/* Somebody else might have mapped it while we slept */
			if (page_address(page))
				return (unsigned long)page_address(page);

			/* Re-start */
			goto start;
		}
	}
	vaddr = PKMAP_ADDR(last_pkmap_nr);
	set_pte_at(&init_mm, vaddr,
		   &(pkmap_page_table[last_pkmap_nr]), mk_pte(page, kmap_prot));

	pkmap_count[last_pkmap_nr] = 1;
	set_page_address(page, (void *)vaddr);

	return vaddr;
}
`
kunmap()撤销先前由kmap()建立的永久内核映射。如果页确实在高端内存中，则调用kunmap_high()函数。

`
void kunmap_high(struct page *page)
{
	unsigned long vaddr;
	unsigned long nr;
	unsigned long flags;
	int need_wakeup;

	lock_kmap_any(flags);
	vaddr = (unsigned long)page_address(page);
	BUG_ON(!vaddr);
	nr = PKMAP_NR(vaddr);

	/*
	 * A count must never go down to zero
	 * without a TLB flush!
	 */
	need_wakeup = 0;
	switch (--pkmap_count[nr]) {
	case 0:
		BUG();
	case 1:
		/*
		 * Avoid an unnecessary wake_up() function call.
		 * The common case is pkmap_count[] == 1, but
		 * no waiters.
		 * The tasks queued in the wait-queue are guarded
		 * by both the lock in the wait-queue-head and by
		 * the kmap_lock.  As the kmap_lock is held here,
		 * no need for the wait-queue-head's lock.  Simply
		 * test if the queue is empty.
		 */
		need_wakeup = waitqueue_active(&pkmap_map_wait);
	}
	unlock_kmap_any(flags);

	/* do wake-up, if needed, race-free outside of the spin lock */
	if (need_wakeup)
		wake_up(&pkmap_map_wait);
}
`
### 临时内核映射
临时内核映射可以用在中断处理程序和可延迟函数的内部，因为它们从不阻塞进程。

enum fixed_address数据结构中包含符号FIX_KMAP_BEGIN和FIX_KMAP_END,

`
#ifdef CONFIG_X86_32
	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
	FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
`
在这种方式下，系统的每个CPU都有KM_TYPE_NR个固定映射的线性地址。这些线性地址用于建立临时映射。

内核调用kmap_atomic()函数建立临时内核映射。

`
void *kmap_atomic(struct page *page)
{
	return kmap_atomic_prot(page, kmap_prot);//在kmap_init()中将kmap_prot初始化为PAGE_KERNEL。

}
`
kmap_atomic_prot()函数定义如下：

`
void *kmap_atomic_prot(struct page *page, pgprot_t prot)
{
	unsigned long vaddr;
	int idx, type;

	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */
	pagefault_disable();

	if (!PageHighMem(page))
		return page_address(page);

	type = kmap_atomic_idx_push();
	idx = type + KM_TYPE_NR*smp_processor_id();
	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);
	BUG_ON(!pte_none(*(kmap_pte-idx)));
	set_pte(kmap_pte-idx, mk_pte(page, prot));
	arch_flush_lazy_mmu_mode();

	return (void *)vaddr;
}
`
### 伙伴算法

### 每CPU页框高速缓存
为了提升系统性能，每个**内存管理区**定义了一个“每CPU”页框高速缓存。所有“每CPU”高速缓存包含一些预先分配的页框，它们被用于满足本地CPU发出的单一内存请求。

每CPU页框高速缓存其实分为两种，一种是热高速缓存，一种叫冷高速缓存。

热高速缓存存放的页框中所包含的内容很可能就在CPU硬件高速缓存中。如果内核或用户态进程在刚分配到页框就立即向页框写，那么从热高速缓存中获得页框就对系统性能有利。

反过来，如果页框将要被DMA操作填充，那么从冷高速缓存中获得页框比较合适。因为在这种情况下，不会涉及CPU，硬件高速缓存的行也不会被修改，没必要使用热高速缓存。它们从冷高速缓存中获得页框，就可以为其他类型的内存分配节省热页框的储备。

#### 数据结构
在内存管理区描述符中，有一个pageset字段,它其实是一个per_cpu_pageset数组。

`
struct per_cpu_pageset __percpu *pageset;
`
该数组包含为每个CPU提供的一个元素，每个元素由两个per_cpu_pages描述符组成，一个留给热高速缓存，一个留给冷高速缓存。（在2.6.11中可能是这样，但在3.10.0中，被合成一个了）

`
struct per_cpu_pageset {
	struct per_cpu_pages pcp;
#ifdef CONFIG_NUMA
	s8 expire;
#endif
#ifdef CONFIG_SMP
	s8 stat_threshold;
	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
#endif
};
`

`
struct per_cpu_pages {
	int count;		/* number of pages in the list */
	int high;		/* high watermark, emptying needed */
	int batch;		/* chunk size for buddy add/remove */

	/* Lists of pages, one per migrate type stored on the pcp-lists */
	struct list_head lists[MIGRATE_PCPTYPES];
};

`
### 通过每CPU页框高速缓存分配页框
buffered_rmqueue()函数在指定的内存管理区中分配页框。
其中有几句比较重要：
`
unsigned long flags;
	struct page *page;
	bool cold = ((gfp_flags & __GFP_COLD) != 0);

again:
	if (likely(order == 0)) {//单个页框，通过高速缓存分配，否则通过伙伴系统
		struct per_cpu_pages *pcp;
		struct list_head *list;

		local_irq_save(flags);
		pcp = &this_cpu_ptr(zone->pageset)->pcp;
		list = &pcp->lists[migratetype];
		if (list_empty(list)) {
			pcp->count += rmqueue_bulk(zone, 0,
					pcp->batch, list,
					migratetype, cold);
			if (unlikely(list_empty(list)))
				goto failed;
		}

		if (cold)
			page = list_entry(list->prev, struct page, lru);
		else
			page = list_entry(list->next, struct page, lru);

		list_del(&page->lru);
		pcp->count--;
	}
	
`
可以看出，在3.10.0中，如果需要冷高速缓存，就用list->prev,否则用list->next。
### 释放页框到每CPU页框高速缓存

free_hot_cold_page()

`

void free_hot_cold_page(struct page *page, bool cold)
{
	struct zone *zone = page_zone(page);
	struct per_cpu_pages *pcp;
	unsigned long flags;
	int migratetype;

	if (!free_pages_prepare(page, 0))
		return;

	migratetype = get_pageblock_migratetype(page);
	set_freepage_migratetype(page, migratetype);
	local_irq_save(flags);
	__count_vm_event(PGFREE);

	/*
	 * We only track unmovable, reclaimable and movable on pcp lists.
	 * Free ISOLATE pages back to the allocator because they are being
	 * offlined but treat RESERVE as movable pages so we can get those
	 * areas back if necessary. Otherwise, we may have to free
	 * excessively into the page allocator
	 */
	if (migratetype >= MIGRATE_PCPTYPES) {
		if (unlikely(is_migrate_isolate(migratetype))) {
			free_one_page(zone, page, 0, migratetype);
			goto out;
		}
		migratetype = MIGRATE_MOVABLE;
	}

	pcp = &this_cpu_ptr(zone->pageset)->pcp;
	if (!cold)
		list_add(&page->lru, &pcp->lists[migratetype]);
	else
		list_add_tail(&page->lru, &pcp->lists[migratetype]);
	pcp->count++;
	if (pcp->count >= pcp->high) {
		unsigned long batch = ACCESS_ONCE(pcp->batch);
		free_pcppages_bulk(zone, batch, pcp);
		pcp->count -= batch;
	}

out:
	local_irq_restore(flags);
}
`
## 管理区分配器
